{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9efb5674-44f0-433e-8133-96ff2123b337",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "#sc = pyspark.SparkContext(appName = \"MyAPP\")\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "#spark = SparkSession(sc)\n",
    "import sys\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "#from pyspark.sql.functions import udf,col\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import  StringIndexer, VectorAssembler,StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4d08ab8d-df14-452b-9a7e-b8951e27561c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sparkConf = SparkConf().setMaster(\"local\").setAppName(\"MongoSparkConnectorTour\").set(\"spark.app.id\", \"MongoSparkConnectorTour\")\n",
    "# sqlContext = SQLContext(sc)\n",
    "# # create and load dataframe from MongoDB URI\n",
    "# df1 = sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "#                     .option(\"spark.mongodb.input.uri\", \"mongodb+srv://admin:12345678Pc@cluster0.k5xvg.mongodb.net/2020MSBD5003GP11.BehaviorData?retryWrites=true&w=majority\")\\\n",
    "#                     .load()\n",
    "\n",
    "# # print data frame schema\n",
    "# df1.printSchema()\n",
    "# df = df1.limit(5000)\n",
    "df = spark.read.csv('../data/2019-Oct.csv', header=True, inferSchema=True).limit(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "915bec6e-1b3a-4e09-8add-773f4c39c950",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Need Some Plot to show large Occurence of these types of data\n",
    "#drop rows with null value on category_code and brand\n",
    "df = df.where(\"category_code is not null and brand is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2500564a-fabf-4403-8050-2b0a868a855e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#event_type count during each user_session \n",
    "df_activity = df.groupBy('user_session').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e28d5bc-b094-4815-9a33-6618130fa1a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Need Some Plot to show large Occurence of these types of data\n",
    "#select rows of event_type = 'cart' or event_type = 'purchase'\n",
    "#drop duplicates rows based on ['event_type', 'product_id','price', 'user_id','user_session']\n",
    "df = df.select('*').where(\"event_type = 'cart' or event_type = 'purchase'\").dropDuplicates(subset = ['event_type', 'product_id','price', 'user_id','user_session'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8ca4309e-51a6-448e-8d10-3ccfb67d37b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#add new column is_purchased if event_type = purchase then 1 else 0\n",
    "df = df.withColumn('is_purchased', when(col('event_type') == 'purchase', 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "13a8cdd7-663e-4d3d-8ee2-22bc7bd580d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#chop the dataset into 2 sets by event_type\n",
    "df_cart = df.select('*').where(\"event_type = 'cart'\")\n",
    "df_purchase = df.select('*').where(\"event_type = 'purchase'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc4ea4fc-f100-4d53-bb96-04f7dd892176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#left join rows of cart with rows of purchase, update the is_purchased of cart to is_purchased of corresponding purchase with key ['user_session', 'product_id']\n",
    "#only consider goods put into cart and be purchased afterward\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df_cart_purchase=df_cart.alias('a').join(\n",
    "    df_purchase.alias('b'), ['user_session', 'product_id'], how='left'\n",
    ").select('user_session', 'product_id', 'a.event_time', 'a.event_type', 'a.product_id', 'a.category_id', 'a.category_code', 'a.brand', 'a.price', 'a.user_id',\n",
    "    f.coalesce('b.is_purchased', 'a.is_purchased').alias('is_purchased')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23fa1a83-db70-4e40-9c11-4dda3c629aec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#join the activity_count\n",
    "df_cart_purchase=df_cart_purchase.join(df_activity, 'user_session') \\\n",
    "  .select('*') \\\n",
    "  .withColumnRenamed('count', 'activity_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0850f079-188b-4388-a5e2-2c3fff6a2430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#split the category_code by .\n",
    "df_cart_purchase = df_cart_purchase.select('*', split('category_code',\"\\\\.\")[0], split('category_code',\"\\\\.\")[1], split('category_code',\"\\\\.\")[2])\\\n",
    "  .withColumnRenamed('split(category_code, \\., -1)[0]', 'category_code_level1') \\\n",
    "  .withColumnRenamed('split(category_code, \\., -1)[1]', 'category_code_level2') \\\n",
    "  .withColumnRenamed('split(category_code, \\., -1)[2]', 'category_code_level4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "68a9f320-f796-44cb-a63b-5940c7e2830b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#add new field week_day and category_code_level3 to category_code_level2 if category_code_level4 is null\n",
    "df_cart_purchase = df_cart_purchase.select('*', dayofweek(col('event_time')).alias('week_day'), f.coalesce('category_code_level2', 'category_code_level4').alias('category_code_level3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "607c6e03-81f8-4fef-9fdf-b3f6199a54f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Index category / String features\n",
    "indexer = StringIndexer(inputCols=['brand', 'category_code_level1', 'category_code_level2','category_code_level3'], outputCols=['brandIndex', 'category_code_level1_index', 'category_code_level2_index','category_code_level3_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector Assembler - all numeric features\n",
    "assembler = VectorAssembler()\\\n",
    "         .setInputCols ([\"product_id\",\"category_id\",\"brandIndex\",\n",
    "                         \"price\",\"week_day\",\"user_id\",\\\n",
    "                         \"category_code_level1_index\",\"category_code_level2_index\",\"category_code_level3_index\", \"activity_count\"])\\\n",
    "         .setOutputCol (\"vectorized_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabelIndex - the label is_purchased\n",
    "label_indexer = StringIndexer()\\\n",
    "         .setInputCol (\"is_purchased\")\\\n",
    "         .setOutputCol (\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize numerical features \n",
    "scaler = StandardScaler()\\\n",
    "         .setInputCol (\"vectorized_features\")\\\n",
    "         .setOutputCol (\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Pipeline of transformers\n",
    "pipeline_stages=Pipeline()\\\n",
    "                .setStages([indexer,assembler,label_indexer,scaler])\n",
    "pipeline_model=pipeline_stages.fit(df_cart_purchase)\n",
    "pipeline_df=pipeline_model.transform(df_cart_purchase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select Data for Modeling\n",
    "selectedCols = original_columns + ['features', 'label'] \n",
    "pipeline_df = pipeline_df.select(selectedCols)\n",
    "pipeline_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 294\n",
      "Test Dataset Count: 78\n"
     ]
    }
   ],
   "source": [
    "#Split train and Test data\n",
    "train, test = pipeline_df.randomSplit([0.8, 0.2], seed = 7)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Logistic Regression Model and see the data\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=5)\n",
    "lrModel = lr.fit(train)\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions.select('label', 'features',  'rawPrediction', 'prediction', 'probability').toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.6410256410256411\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of the Raw Model\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "print(\"Accuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.6410256410256411\n"
     ]
    }
   ],
   "source": [
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 2 values for lr.regParam, 3 values of maxIter\n",
    "# this grid will have 3 x 2  = 6 parameter settings for CrossValidator to choose from.\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.maxIter, [1, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)  \n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "predictions_cv = cvModel.transform(test)\n",
    "\n",
    "accuracy_cv = predictions_cv.filter(predictions_cv.label == predictions_cv.prediction).count() / float(predictions_cv.count())\n",
    "print(\"Accuracy : \",accuracy_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "EDA2",
   "notebookOrigID": 1776979405046581,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
